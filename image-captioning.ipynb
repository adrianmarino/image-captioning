{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import Input, layers, optimizers\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Embedding, Dense,Dropout, LeakyReLU\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from lib.data.flickr_dataset import FlickrDataset\n",
    "from lib.data.feature_vector_generator import FeatureVectorGenerator\n",
    "from lib.data.glove_word_embedding_generator import GloveWordEmbeddingGenerator\n",
    "from lib.data.generator.data_generator import DataGenerator\n",
    "\n",
    "from lib.model.model_wrapper import ModelWrapper\n",
    "from lib.model.metrics import rmse\n",
    "from lib.model.greedy_search import GreedySearch\n",
    "\n",
    "from lib.utils.word_utils import word_to_index_and_index_to_word\n",
    "from lib.utils.file_utils import create_directory\n",
    "from lib.utils.plot_utils import show_sample\n",
    "from lib.utils.array_utils import column\n",
    "from lib.utils.pickle_utils import save_obj, load_obj\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from lib.callback.metric_plotter import MetricsPlotter\n",
    "from lib.callback.adam_learning_rate_tracker import AdamLearningRateTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name='8k'\n",
    "# dataset_name='30k'\n",
    "data_path = f'./dataset/{dataset_name}/data'\n",
    "images_path = f'./dataset/{dataset_name}/images'\n",
    "image_features_path = f'./dataset/{dataset_name}/img_features.pkl'\n",
    "word_embedding_path = './dataset/glove.6B.200d.txt'\n",
    "line_separator = { \n",
    "    '8k': r'#[0-9]',\n",
    "    '30k': r'\\| [0-9]\\|'\n",
    "}\n",
    "weights_path = create_directory(f'weights/{dataset_name}')\n",
    "weights_file_path_patern = weights_path + '/weights__epoch_{epoch:02d}__loss_{val_loss:.4f}__acc_{val_rmse:.4f}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max len desc: $ an africanamerican man wear green sweatshirt and blue vest be hold up dollar bill in front of his face while stand on busy sidewalk in front of group of man play instrument #\n"
     ]
    }
   ],
   "source": [
    "dataset = FlickrDataset(\n",
    "    data_path, \n",
    "    images_path,\n",
    "    desc_prefix='$', \n",
    "    desc_postfix='#',\n",
    "    clean_desc=True,\n",
    "    separator=line_separator[dataset_name]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5663, Val: 2185, Test: 243\n"
     ]
    }
   ],
   "source": [
    "train_samples, remain_samples = train_test_split(dataset.samples(), test_size=0.3)\n",
    "val_samples, test_samples = train_test_split(remain_samples, test_size=0.1)\n",
    "print(f'Train: {len(train_samples)}, Val: {len(val_samples)}, Test: {len(test_samples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./dataset/8k/images/3298175192_bbef524ddc.jpg',\n",
       " ['$ black and white dog be chew on camera #',\n",
       "  '$ black and white dog rest its head on camera #',\n",
       "  '$ puppy play with camera #',\n",
       "  '$ black and white dog chew on canon camera set in the grass the camera be black and white #',\n",
       "  '$ black and white puppy gnaws camera #'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('./dataset/8k/images/1305564994_00513f9a5b.jpg', array([ 0.06508786,  0.03218709,  0.0237698 , ...,  0.3836866 ,\n",
      "        0.19910577,  0.23510413], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(image_features_path):\n",
    "    image_paths = dataset.samples(col=0)\n",
    "    image_features = list(FeatureVectorGenerator().generate(image_paths))\n",
    "    save_obj(image_features_path, image_features)\n",
    "\n",
    "image_features = load_obj(image_features_path)\n",
    "print(image_features[0])\n",
    "image_features = dict(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words(occurs>=10): 1593/6688\n"
     ]
    }
   ],
   "source": [
    "min_occurs=10\n",
    "vocabulary = dataset.words_set(min_occurs=10)\n",
    "vocabulary_size = len(vocabulary)\n",
    "complete_vocabulary_size = len(dataset.words_set())\n",
    "print(f'Words(occurs>={min_occurs}): {vocabulary_size}/{complete_vocabulary_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word = word_to_index_and_index_to_word(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1593"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.max_desc_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_dim = 200\n",
    "embedding_generator = GloveWordEmbeddingGenerator(word_embedding_path, embedding_vector_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = embedding_generator.generate(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocabulary_size, embedding_vector_dim, embedding_matrix, optimizer):\n",
    "    img_feat_input = Input(name=\"Image_Feature\", shape=(2048,))\n",
    "\n",
    "    img_branch = Dropout(0.5)(img_feat_input)\n",
    "    img_branch = Dense(512)(img_branch)\n",
    "    img_branch = LeakyReLU(alpha=0.3)(img_branch)\n",
    "\n",
    "    seq_input = Input(name=\"Description_Sequence\", shape=(dataset.max_desc_len(),))\n",
    "\n",
    "    seq_branch = Embedding(vocabulary_size, embedding_vector_dim, mask_zero=True)(seq_input)\n",
    "    seq_branch = Dropout(0.5)(seq_branch)\n",
    "    seq_branch = LSTM(512)(seq_branch)\n",
    "\n",
    "    decoder = add([img_branch, seq_branch])\n",
    "    decoder = Dense(512)(decoder)\n",
    "    decoder = LeakyReLU(alpha=0.3)(decoder)\n",
    "    \n",
    "    outputs = Dense(name=\"Words_Distribution\", units=vocabulary_size, activation='softmax')(decoder)\n",
    "\n",
    "    model = Model(inputs=[img_feat_input, seq_input], outputs=outputs)\n",
    "\n",
    "    model.layers[3].set_weights([embedding_matrix])\n",
    "    model.layers[3].trainable = False\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=optimizer,\n",
    "        metrics=[rmse]\n",
    "    )\n",
    "\n",
    "    return ModelWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    weights_file_path_patern,\n",
    "    monitor='val_loss', \n",
    "    verbose=1,\n",
    "    save_best_only=True, \n",
    "    save_weights_only=True, \n",
    "    mode='auto',\n",
    "    period=1\n",
    ")\n",
    "lr_tracker = AdamLearningRateTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs, batch_size, lr = 6, 10, 0.001\n",
    "epochs, batch_size, lr = 6, 10, 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(\n",
    "    train_samples,\n",
    "    image_features,\n",
    "    word_to_index,\n",
    "    index_to_word,\n",
    "    dataset.max_desc_len(), \n",
    "    vocabulary_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_generator = DataGenerator(\n",
    "    val_samples,\n",
    "    image_features,\n",
    "    word_to_index,\n",
    "    index_to_word,\n",
    "    dataset.max_desc_len(), \n",
    "    vocabulary_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocabulary_size, embedding_vector_dim, embedding_matrix, Adam(lr=lr))\n",
    "# model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(f'{weights_path}/weights__epoch_05__loss_3.1049__acc_0.0223.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    val_generator,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=len(train_samples)/batch_size,\n",
    "    callbacks=[\n",
    "        checkpoint,\n",
    "        lr_tracker,\n",
    "        MetricsPlotter(val_generator, plot_interval=100, evaluate_interval=100, batch_size=batch_size)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GreedySearch(model, word_to_index, index_to_word, '$', '#', dataset.max_desc_len())\n",
    "\n",
    "import random\n",
    "image_path = test_samples[random.randint(0, len(test_samples)-1)][0]\n",
    "\n",
    "show_sample(\n",
    "    image_path,\n",
    "    description=search.perform( image_features[image_path])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
